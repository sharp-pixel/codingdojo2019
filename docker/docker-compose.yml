version: '3.4'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:$CONFLUENT_VERSION
    # ports:
    #   - 2181:2181
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_JMX_PORT: 9000
      ZOOKEEPER_SERVER_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    healthcheck:
      test: jps | grep QuorumPeerMain || exit 1
      interval: 300s
      timeout: 10s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - zookeeper

  broker:
    image: confluentinc/cp-kafka:$CONFLUENT_VERSION
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # For Exactly once with ONE broker
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # For Exactly once with ONE broker
      KAFKA_JMX_PORT: 9000
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    healthcheck:
      test: jps | grep SupportedKafka || exit 1
      interval: 300s
      timeout: 10s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - kafka
      - zookeeper

  kafka-client:
    image: confluentinc/cp-kafka:$CONFLUENT_VERSION
    depends_on:
      - broker
    environment:
      # Dummy values to satisfy image requirements
      KAFKA_ZOOKEEPER_CONNECT: ignored
      KAFKA_ADVERTISED_LISTENERS: ignored
    command:
      "bash -c -a 'echo Waiting for Kafka to be ready... && \
                       /etc/confluent/docker/configure && \
                       cub kafka-ready -b kafka-broker:9092 1 60 --config /etc/kafka/kafka.properties && \
                       sleep 5 && 
                       kafka-topics --zookeeper zookeeper:2181 --topic events --create --partitions 3 --replication-factor 1 && \
                       kafka-topics --zookeeper zookeeper:2181 --topic users --create --partitions 3 --replication-factor 1 --config cleanup.policy=compact'"
    networks:
      - kafka
      - zookeeper

  schema-registry:
    image: confluentinc/cp-schema-registry:$CONFLUENT_VERSION
    depends_on:
      - broker
    ports:
      - 8081:8081
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka-broker:9092'
      KAFKA_JMX_PORT: 9000
      SCHEMA_REGISTRY_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    # healthcheck:
    #   test: curl -f schema-registry:8081/subjects || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - kafka

  connect-worker:
    image: confluentinc/cp-kafka-connect:$CONFLUENT_VERSION
    depends_on:
      - zookeeper
      - broker
      - schema-registry
    ports:
      - 8083:8083
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      CONNECT_PLUGIN_PATH: '/usr/share/java'
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
      KAFKA_JMX_PORT: 9000
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9000"
    # healthcheck:
    #   test: curl -f kafka-connect:8083/connectors || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    volumes:
      - ./mysql-connector-java-8.0.12.jar:/usr/share/java/kafka-connect-jdbc/mysql-connector-java-8.0.12.jar

    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - kafka
      - zookeeper
      - elastic
      - mysql
  #     - cassandra

  # cassandra:
  #   image: datastax/dse-server:6.0.1
  #   environment:
  #     DS_LICENSE: accept
  #   cap_add:
  #     - IPC_LOCK
  #   ulimits:
  #     memlock: -1
  #   ports:
  #     - 9042:9042
  #   # healthcheck:
  #   #   test: ["CMD-SHELL", "[ $$(nodetool statusgossip) = running ]"]
  #   #   interval: 30s
  #   #   timeout: 10s
  #   #   retries: 5
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "200k"
  #       max-file: "10"
  #   networks:
  #     - cassandra

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:$ELASTICSEARCH_VERSION
    environment:
      discovery.type: 'single-node'
      bootstrap.memory_lock: 'true'
      ES_JAVA_OPTS: '-Xms512m -Xmx512m'
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - 9200:9200
    # healthcheck:
    #   test: curl -f elasticsearch:9200 || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - elastic

  kibana:
    image: docker.elastic.co/kibana/kibana:$ELASTICSEARCH_VERSION
    depends_on:
      - elasticsearch
    ports:
      - 5601:5601
    # healthcheck:
    #   test: curl -f kibana:5601 || exit 1
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "200k"
        max-file: "10"
    networks:
      - elastic

  mysql:
    image: mysql
    environment:
      MYSQL_USER: dojo
      MYSQL_PASSWORD: dojo2018
      MYSQL_ROOT_PASSWORD: dojo2018
    networks:
      - mysql

networks:
  elastic:
  kafka:
  zookeeper:
  mysql:
